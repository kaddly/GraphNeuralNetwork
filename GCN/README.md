# 3.GCN

简单来说，Graph Embedding技术一般通过特定的策略对图中的顶点进行游走采样进而学习到图中的顶点的相似性，可以看做是一种将图的拓扑结构进行向量表示的方法。

然而现实世界中，图中的顶点还包含若干的属性信息，如社交网络中的用户画像信息，引文网络中的文本信息等，对于这类信息，基于GraphEmbedding的方法通常是将属性特征拼接到顶点向量中提供给后续任务使用。

本文介绍的GCN则可以直接通过对图的拓扑结构和顶点的属性信息进行学习来得到任务结果。

**GCN的本质目的就是用来提取拓扑图的空间特征**。 而图卷积神经网络主要有两类，

- 一类是基于**空间域或顶点域vertex domain(spatial domain)**的

- 一类则是基于**频域或谱域spectral domain**的

通俗点解释，空域可以类比到直接在图片的像素点上进行卷积，而频域可以类比到对图片进行傅里叶变换后，再进行卷积。

## 3.1 图卷积

普通卷积神经网络研究的对象是具备Euclidean domains的数据，Euclidean domains data数据最显著的特征是他们具有规则的空间结构，如图片是规则的正方形，语音是规则的一维序列等，这些特征都可以用一维或二维的矩阵来表示，卷积神经网络处理起来比较高效。

CNN的【平移不变性】在【非矩阵结构】数据上不适用

- **平移不变性（translation invariance）**：比较好理解，在用基础的分类结构比如ResNet、Inception给一只猫分类时，无论猫怎么扭曲、平移，最终识别出来的都是猫，**输入怎么变形输出都不变**这就是平移不变性，网络的层次越深这个特性会越明显。
- **平移可变性（translation variance）**：针对目标检测的，比如一只猫从图片左侧移到了右侧，检测出的猫的坐标会发生变化就称为平移可变性。当卷积网络变深后最后一层卷积输出的feature map变小，物体在输入上的小偏移，经过N多层pooling后在最后的小feature map上会感知不到，这就是为什么R-FCN原文会说网络变深平移可变性变差。

**离散卷积本质就是一种加权求和**。CNN中的卷积就是一种离散卷积，本质上就是利用一个共享参数的过滤器（kernel），通过**计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征的提取**，当然**加权系数就是卷积核的权重系数(W)**。

卷积核的系数通过随机化初值，然后根据误差函数通过反向传播梯度下降进行迭代优化。这是一个关键点，卷积核的参数通过优化求出才能实现特征提取的作用，GCN的理论很大一部分工作就是为了引入可以优化的卷积参数。

生活中很多数据不具备规则的空间结构，称为Non Euclidean data，如，推荐系统、电子交易、分子结构等抽象出来的图谱。这些图谱中的每个节点连接不尽相同，有的节点有三个连接，有的节点只有一个连接，是不规则的结构。对于这些不规则的数据对象，普通卷积网络的效果不尽人意。CNN卷积操作配合pooling等在结构规则的图像等数据上效果显著，但是如果作者考虑非欧氏空间比如图(即graph，流形也是典型的非欧结构，这里作者只考虑图)，就难以选取固定的卷积核来适应整个图的不规则性，如邻居节点数量的不确定和节点顺序的不确定。

比如：社交网络非常适合用图数据来表达，如社交网络中节点以及节点与节点之间的关系，用户A（有ID信息等）、用户B、帖子都是节点，用户与用户之间的关系是关注，用户与帖子之间的关系可能是发布或者转发。通过这样一个图谱，可以分析用户对什么人、什么事感兴趣，进一步实现推荐机制。

总结一下，图数据中的空间特征具有以下特点：

1. **节点特征**：每个节点有自己的特征；（体现在点上）
2. **结构特征**：图数据中的每个节点具有结构特征，即节点与节点存在一定的联系。（体现在边上）

总地来说，图数据既要考虑**节点信息**，也要考虑**结构信息**，图卷积神经网络就可以自动化地既**学习节点特征**，又能**学习节点与节点之间的关联信息**。

综上所述，GCN是要为除CV、NLP之外的任务提供一种处理、研究的模型。
**图卷积的核心思想是利用『边的信息』对『节点信息』进行『聚合』从而生成新的『节点表示』。**

## 3.2 拉普拉是矩阵

拉普拉斯矩阵(Laplacian matrix) 也叫做导纳矩阵、**基尔霍夫矩阵**或**离散拉普拉斯算子**，主要应用在图论中，作为一个图的矩阵表示。对于图 G=(V,E)，其Laplacian 矩阵的定义为 L=D-A，其中 L 是Laplacian 矩阵， D=diag(d)是顶点的度矩阵（对角矩阵）,d=rowSum(A)，对角线上元素依次为各个顶点的度， A 是图的邻接矩阵。

Graph Fourier Transformation及Graph Convolution的定义都用到图的拉普拉斯矩阵。

**频域卷积的前提条件是图必须是无向图**，只考虑无向图，那么L就是对称矩阵。

![image-20220706151650867](./img/Laplacian.jpg)

- 普通形式下的拉普拉斯矩阵

$$
L=D-A
$$

L中的元素给定为：
$$
L_{i,j}=\left\{ \begin{array}{l}
	diag(v_i)\quad i=j\\
	-1\quad i\neq j\ and\ v_i\ is\ adjacent\ to\ v_j\\
	0\quad otherwise\\
\end{array} \right.
$$
其中diag(vi) 表示顶点 i 的度。

- 对称归一化的拉普拉斯矩阵（Symmetric normalized Laplacian）

$$
L^{sys}=D^{-1/2}LD^{-1/2}=I-D^{-1/2}AD^{-1/2}
$$

矩阵定义为
$$
L_{i,j}^{sys}=\left\{ \begin{array}{l}
	1\quad i=j\\
	-\frac{1}{\sqrt{diag(v_i)diag(v_j)}}\quad i\neq j\ and\ v_i\ is\ adjacent\ to\ v_j\\
	0\quad otherwise\\
\end{array} \right.
$$
很多GCN的论文中应用的是这种拉普拉斯矩阵

- 随机游走归一化拉普拉斯矩阵（Random walk normalized Laplacian）

$$
L^{rw}=D^{-1}L=I-D^{-1}A
$$

矩阵元素定义
$$
L_{i,j}^{rw}=\left\{ \begin{array}{l}
	1\quad i=j\\
	-\frac{1}{diag(v_i)}\quad i\neq j\ and\ v_i\ is\ adjacent\ to\ v_j\\
	0\quad otherwise\\
\end{array} \right.
$$

- 泛化的拉普拉斯（Generalized Laplacian)

泛化的拉普拉斯(用得少)定义为:
$$
\left\{ \begin{array}{l}
	Q_{i,j}<0\quad i\neq j\ and\ diag(v_i)\neq 0\\
	Q_{i,j}=0\quad i\neq j\ and\ v_i\ is\ adjacent\ to\ v_j\\
	anynumber\quad otherwise\\
\end{array} \right.
$$
标准归一化的拉普拉斯矩阵还是对称的，并且符合前面的公式定义。

Graph Convolution与Diffusion相似之处，当然从Random walk normalized Laplacian就能看出了两者确有相似之处

### 拉普拉斯矩阵的性质

- 拉普拉斯矩阵是个半正定矩阵（最小特征值大于等于0）
- **特征值中0出现的次数就是图连通区域的个数**
- 最小特征值是0，因为拉普拉斯矩阵（普通形式：L=D-A）每一行的和均为0，**并且最小特征值对应的特征向量是每个全为1的向量**
- 最小非零特征值是图的代数连通度。

拉普拉斯矩阵的半正定性证明,如下：
要证明拉普拉斯矩阵是半正定的，只需要证明其二次型$f^TLf\ge0$

<img src="./img/math.jpg" alt="image-20220706154105541" style="zoom:50%;" />

所以，对于任意一个属于实向量$f\in R^m$(f为m*1的实数向量)，都有此公式成立：$f^TLf=\frac{1}{2}\sum^m_{i,j=1}(f_i-f_j)^2$

### GCN用拉普拉斯矩阵的理由

- 拉普拉斯矩阵是对称矩阵，可以进行特征分解（谱分解）
- 由于卷积在傅里叶域的计算相对简单，为了在graph上做傅里叶变换，需要找到graph的连续的正交基对应于傅里叶变换的基，因此要使用拉普拉斯矩阵的特征向量。

