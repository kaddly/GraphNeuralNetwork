# 8. MetaPath2Vec

metapath2vec用于解决异构网络的embedding表示。是将deepwalk的算法思路引入到异构网络当中，并针对异构网络的特点，针对deepwalk算法中的各个步骤，针对性的进行优化。

## 8.1、概念

异构网络不同于同构网络在于，同构网络中的nodes都是同一类型的节点，link都是针对同一类型节点的链接。而异构网络节点可以是不同类型的实体，链接也可以是不同实体之间的链接。拿电商行为来说，用户A购买了店铺S1的商品L1，用户A购买了店铺S2的商品L2。如果我们想用同构网络来表示用户的购买session，那么网络的节点就是可以用商品来表示，并把同一用户的购买商品按顺序链接起来。但是这样就丢失了用户和店铺这两个实体。异构网络就可以解决这个问题，我们可以直接把用户和店铺进行链接，店铺和商品进行链接，用户和商品进行链接起来，形成一个异构网路。

### 定义问题

**异构网络（Heterogeneous Networks）:** 异构网络由图G(V,E,T)表示，其中节点v和链接e分别对应映射函数：

$\phi(v):V->T_v$和$\phi(e):E->T_e$，这里的$T_e$和$T_v$指对象和关系类型的集合。并且$|T_e|+|T_v|\gt 2$。如果等于2的话就是同构网络。这里想要说明的意思其实就是说 异构网络是由一个或者多个类型的节点，以及一个或者多个类型的链接组成的图。

**异构网络表示学习(Heterogeneous Network Reperesentation Learning):**给定一个异构网络G，这里的工作就是将图中的节点v表示成一个低纬向量$X\in R^{|V|*d},d\lt\lt|V|$，用于学习出图中节点的结构和语义相关性。而该问题的输出则为低纬矩阵X，表示的是一个所有节点的低纬向量，所以它的大小是节点个数*d。而该问题因为同时优化不同类型的节点，所以最后生成的节点低纬向量都在同一高纬空间。所以我们能很方便的将学习出的向量应用于各种各样的任务。

## 8.2、异构网络embedding：metapath2vec

在metapath2vec中，采用和deepwalk类似的方式，利用skip-gram来学习图的embedding。主要步骤由两步，

1. 利用随机游走来从图中获取序列，
2. 利用skip-gram优化提取的序列。但是针对异构图，这部分中都存在一定的差异。这里分别说一下。

**Heterogeneous Skip-Gram. 给定**异构网络$G=(V,E,t),|T_V|\gt 1$，目标就是在给定节点v后，使其上下文内容存在的概率最大化，如下：
$$
arg max_{\theta}\sum_{v\in V}\sum_{t\in T_V}\sum_{c_t\in N_t(v)}\log{p(c_t|v;\theta)}
$$
这里的$N_t(v)$指的是在节点v的邻近节点中，为第t个类型的节点。而概率函数$p(c_t|v;\theta)$则为softmax。可表示为：
$$
p(c_t|v;\theta)=\frac{e^{X_{c_t}{X_v}}}{\sum_{u\in V}e^{X_uX_v}}
$$
这里$X_v$就是从矩阵X从取出来的第v行向量，它表示节点v的embedding向量。为了减少计算量，进一步优化为负采样后的优化目标：
$$
\log{\sigma(X_{c_t}X_v)}+\sum_{m=1}^ME_{u^m}P(v)[\log{\sigma(-X_{u^m}X_v)}]
$$
其中是sigmoid函数，p(u)是预定义的函数，用于采样节点M次。这里它并没有区分不同的节点来进行采样，对不同节点进行均匀采样。

**元路径随机游走（Meta-Path-Based Random Walks.）**在deepwalk和node2vec中都采用的是随机游走的方式，如果不考虑节点之间的类型，我们也可以直接使用随机游走来定义转化概率$p(v^{i+1}|v^i)$,然后生成的序列直接利用skipgram来进行优化。但是sun等人证明来随机游走是偏向于高可见类型的节点，即那些具有主导优势的节点，所以本文提出来一个基于元路径的随机游走方法。

定义metapath的P的形式为
$$
v_1\to^{R_1} v_2\to^{R_2}...\to^{R_t}v_t
$$
其中$v_1,v_2$指的都是节点类型，metapath的意思就是事先定义好节点类型的变化规律。而整个变化规律就是从$v_1$到$v_t$这样就可以将变化规律限制在定义好的metapath之中，用于解决随机游走偏向于选择高可见类型节点的问题。具体的在每步的转移概率为：
$$
p(v^{i+1}|v_t^i,\rho)=\left\{ \begin{array}{l}
	\frac{1}{|N_{t+1}(v_t^i)|}\quad (v^{i+1},v_t^i)\in E,\phi(v^{i+1})=t+1\\
	0\quad (v^{i+1},v_t^i)\in E,\phi(v^{i+1})\neq t+1\\
	0\quad (v^{i+1},v_t^i)\in E\\
\end{array} \right.
$$
其中$v_t^i\in V_t$，而$N_{t+1}(v_t^i)$指的是节点$v_t^i$的$V_{t+1}$类型的邻近节点，而转移概率就是该类型节点个数的倒数。表达的是只有在下一步为指定mtapath位置上的节点类型的时候才发生转移，并且转移概率为领域内该类型节点数的倒数。基于metapath的随机游走保证来语义变化的正确性。比如如下图中的a4所示，如果其上一步为CMU，那么它可以转移到a2,a3,a5,p2,p3,CMU中任意和其由链接的节点。但是如果定义来metapath “OAPVPAO”，随机游走就会偏向于选择P类型的节点。


## 8.3、metapath++

在前面的metapath中，存在一个小问题，就是随机游走这一步是考虑来节点的类型了，但是skipgram在训练的时候并有没对其区分对待。所以进一步针对这点提出了**Heterogeneous negative sampling.** 优化函数如下：

$$
p(c_t|v;\theta)=\frac{e^{X_{c_t}X_v}}{\sum_{u_t\in V_t}e^{X_{u_t}X_v}}
$$
$V_t$是整个网络中类型为t的集合，即根据同类型的节点进行softmax归一化。这样的话，相当于每个类型的节点都会有了一个概率分布。按文章给的例子来说，给定一个目标节点a4，那么利用这个方法就能得到对应的V,A,O,P四种节点类型的分布。

优化后的负采样目标如下：
$$
O(X)=\log\sigma(X_{C_t}X_v)+\sum_{m=1}^M\Bbb{E}_{u_t^m\sim P_t(u_t)}[\log\sigma(-X_{u_t^m}X_v)]
$$
