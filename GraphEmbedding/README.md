# 2. Graph Embedding

我们都知道在数据结构中，图是一种基础且常用的结构。现实世界中许多场景可以抽象为一种图结构，如社交网络，交通网络，电商网站中用户与物品的关系等。

目前提到图算法一般指：

1. 经典数据结构与算法层面的：最小生成树(Prim,Kruskal,…)，最短路(Dijkstra,Floyed,…)，拓扑排序，关键路径等
2. 概率图模型，涉及图的表示，推断和学习，详细可以参考Koller的书或者公开课
3. 图神经网络，主要包括Graph Embedding(基于随机游走)和Graph CNN(基于邻居汇聚)两部分。

这里先看下Graph Embedding的相关内容。

Graph Embedding技术将图中的节点以低维稠密向量的形式进行表达，要求在原始图中相似(不同的方法对相似的定义不同)的节点其在低维表达空间也接近。得到的表达向量可以用来进行下游任务，如节点分类，链接预测，可视化或重构原始图等。

## 2.1 DeepWalk

```
DeepWalk是KDD 2014的工作，但却是我们了解Graph Embedding无法绕过的一个方法。
我们都知道在NLP任务中，word2vec是一种常用的word embedding方法，
word2vec通过语料库中的句子序列来描述词与词的共现关系，进而学习到词语的向量表示。
```

### 随机游走

DeepWalk的思想类似word2vec，使用**图中节点与节点的共现关系**来学习节点的向量表示。那么关键的问题就是如何来描述节点与节点的共现关系，DeepWalk给出的方法是使用随机游走(RandomWalk)的方式在图中进行节点采样。RandomWalk是一种**可重复访问已访问节点的深度优先遍历**算法。给定当前访问起始节点，从其邻居中随机采样节点作为下一个访问节点，重复此过程，直到访问序列长度满足预设条件。

![image-20220609083329064](./img/DeepWalkRW.jpg)

### 跳元模型

获取足够数量的节点访问序列后，使用skip-gram model (跳元模型）进行向量学习。

![image-20220609083344163](./img/SkipGram.jpg)

## 2.2 LINE

```
DeepWalk使用DFS随机游走在图中进行节点采样，使用word2vec在采样的序列学习图中节点的向量表示。
LINE也是一种基于邻域相似假设的方法，只不过与DeepWalk使用DFS构造邻域不同的是，LINE可以看作是一种使用BFS构造邻域的算法。
此外，LINE还可以应用在带权图中(DeepWalk仅能用于无权图)。
```

不同的graph embedding方法的一个主要区别是对**图中顶点之间的相似度的定义**不同，所以先看一下LINE对于相似度的定义。

![image-20220609084736300](.\img\LINE.jpg)

### 一阶相似(first-order proximity)

1阶相似度用于描述图中成对顶点之间的局部相似度，形式化描述为若$u,u$之间存在边连接，则边权$w_{uv}$即为两个顶点的相似度，若不存在直接的边，则1阶相似度为0.

eg. 6和7之间存在直连边，且边权较大，则认为两者相似且1阶相似度较高，而5和6之间不存在直连边，则两者间1阶相似度为0。

### 二阶相似(second-order proximity)

仅有1阶相似度就够了吗？显然不够，如上图，虽然5和6之间不存在直连边，但是他们有很多相同的邻居顶点(1,2,3,4)，这其实也可以表明5和6是相似的，而2阶相似度就是用来描述这种关系的。
形式化定义为
$$
p_u=(w_{u,1},...,w_{u,|v|})
$$
其中，$p_u$表示顶点$u$与其他所有顶点之间的1阶相似度，则$u$与$v$的2阶相似度可以通过$p_u$和$p_v$的相似度表示。若$u$和$v$不存在相同的邻居节点，则2阶相似度为0.

### 优化目标

- 1st

对于每一条无向边(i,j)，定义顶点$v_i$和$v_j$之间的联合概率为：
$$
p_1(v_i,v_j)=\frac{1}{1+\exp(-\vec{u}_i^T\cdot \vec{u}_i)}
$$
$\vec{u}_i$为顶点$v_i$的低维向量表示（可以看作一个内积模型，计算两个item之间的匹配程度）

同时定义经验分布
$$
\overset{\land}{p_1}=\frac{w_{ij}}{W}\\W=\sum_{(i,j)\in E}{w_{ij}}
$$
优化目标为最小化:$O_1=d(\overset{\land}{p_1}(.,.),p_1(.,.))$，$d(.,.)$是两个分布的距离，常用的衡量两个概率分布差异的指标为KL散度，使用KL散度并忽略常数项后有
$$
O_1=-\sum_{(i,j)\in E}w_{ij}\log p_1(v_i,v_j)
$$
**1st order 相似度只能用于无向图当中。**

- 2nd

这里对于每个顶点维护两个embedding向量，一个是该顶点本身的表示向量，一个是该点作为其他顶点的上下文顶点时的表示向量。(跟word2vec类似)

对于有向边(i,j)，定义给定$v_j$条件下，生成上下文邻居顶点$v_j$的概率为
$$
p_2(v_i|v_j)=\frac{\exp(\vec{c_j}^T\cdot \vec{u_i})}{\sum^{|V|}_{k=1}\exp(\vec{c_k}^T\cdot \vec{u_i})}
$$
其中，$|V|$为上下文顶点的个数。

优化目标为$O=\sum_{i\in V}\lambda _id(\overset{\land}{p_2}(.|v_i),p_2(.|v_i))$，其中$\lambda _i$为控制节点重要性的因子，可以通过顶点的度数或则PageRank等方法估计得到

经验分布定义为$\overset{\land}{p_2}(v_j|v_i)=\frac{w_{ij}}{d_i}$,$w_{ij}$为边(i,j)的边权，$d_i$是顶点$v_i$的出度，对于带权图，$d_i=\sum_{k\in N(i)}W_{ik}$

同时使用KL散度并设$\lambda _i=d_i$，忽略常数项，有
$$
O_2=-\sum_{(i,i)\in E}w_{ij}\log p_2(v_i|v_j)
$$

### 优化技巧

- Negative sampling

由于计算2阶相似度时，softmax函数的分母计算需要遍历所有顶点，这是非常低效的，论文采用了负采样优化的技巧，目标函数变为：
$$
\log\delta(\vec{c_j}^T\cdot \vec{u_i})+\sum^K_{i=1}E_{v_n\sim P_n(v)}[-\log\delta(\vec{c_n}\cdot \vec{u_i})]
$$
其中，k为负边的个数，论文使用$P_n(v)\propto d^{3/4}_v$,$d_v$是顶点$v$的出度

- Edge Sampling

注意到我们的目标函数在log之前还有一个权重系数$w_{ij}$,在使用梯度下降方法优化参数时，$w_{ij}$会直接乘在梯度上。如果图中的边权方差很大，则很难选择一个合适的学习率。若使用较大的学习率那么对于较大的边权可能会引起梯度爆炸，较小的学习率对于较小的边权则会导致梯度过小。

对于上述问题，如果所有边权相同，那么选择一个合适的学习率会变得容易。这里采用了将带权边拆分为等权边的一种方法，假如一个权重为$w$的边，则拆分后为$w$个权重为1的边。这样可以解决学习率选择的问题，但是由于边数的增长，存储的需求也会增加。

另一种方法则是从原始的带权边中进行采样，每条边被采样的概率正比于原始图中边的权重，这样既解决了学习率的问题，又没有带来过多的存储开销。

这里的采样算法使用的是Alias算法，Alias是一种O(1)时间复杂度的离散事件抽样算法。

### 其他问题

对于新加入图的顶点$v_i$ ，若该顶点与图中顶点存在边相连，我们只需要固定模型的其他参数，优化如下两个目标之一即可：
$$
-\sum_{i\in N(i)}w_{ij}\log p_1(v_j,v_i)
$$
若不存在边相连，则需要利用一些side info，留到后续工作研究。