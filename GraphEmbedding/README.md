# 2. Graph Embedding

我们都知道在数据结构中，图是一种基础且常用的结构。现实世界中许多场景可以抽象为一种图结构，如社交网络，交通网络，电商网站中用户与物品的关系等。

目前提到图算法一般指：

1. 经典数据结构与算法层面的：最小生成树(Prim,Kruskal,…)，最短路(Dijkstra,Floyed,…)，拓扑排序，关键路径等
2. 概率图模型，涉及图的表示，推断和学习，详细可以参考Koller的书或者公开课
3. 图神经网络，主要包括Graph Embedding(基于随机游走)和Graph CNN(基于邻居汇聚)两部分。

这里先看下Graph Embedding的相关内容。

Graph Embedding技术将图中的节点以低维稠密向量的形式进行表达，要求在原始图中相似(不同的方法对相似的定义不同)的节点其在低维表达空间也接近。得到的表达向量可以用来进行下游任务，如节点分类，链接预测，可视化或重构原始图等。

## 2.1 DeepWalk

```
DeepWalk是KDD 2014的工作，但却是我们了解Graph Embedding无法绕过的一个方法。
我们都知道在NLP任务中，word2vec是一种常用的word embedding方法，
word2vec通过语料库中的句子序列来描述词与词的共现关系，进而学习到词语的向量表示。
```

### 随机游走

DeepWalk的思想类似word2vec，使用**图中节点与节点的共现关系**来学习节点的向量表示。那么关键的问题就是如何来描述节点与节点的共现关系，DeepWalk给出的方法是使用随机游走(RandomWalk)的方式在图中进行节点采样。RandomWalk是一种**可重复访问已访问节点的深度优先遍历**算法。给定当前访问起始节点，从其邻居中随机采样节点作为下一个访问节点，重复此过程，直到访问序列长度满足预设条件。

![image-20220609083329064](./img/DeepWalkRW.jpg)

### 跳元模型

获取足够数量的节点访问序列后，使用skip-gram model (跳元模型）进行向量学习。

![image-20220609083344163](./img/SkipGram.jpg)

## 2.2 LINE

```
DeepWalk使用DFS随机游走在图中进行节点采样，使用word2vec在采样的序列学习图中节点的向量表示。
LINE也是一种基于邻域相似假设的方法，只不过与DeepWalk使用DFS构造邻域不同的是，LINE可以看作是一种使用BFS构造邻域的算法。
此外，LINE还可以应用在带权图中(DeepWalk仅能用于无权图)。
```

不同的graph embedding方法的一个主要区别是对**图中顶点之间的相似度的定义**不同，所以先看一下LINE对于相似度的定义。

![image-20220609084736300](./img/LINE.jpg)

### 一阶相似(first-order proximity)

1阶相似度用于描述图中成对顶点之间的局部相似度，形式化描述为若$u,u$之间存在边连接，则边权$w_{uv}$即为两个顶点的相似度，若不存在直接的边，则1阶相似度为0.

eg. 6和7之间存在直连边，且边权较大，则认为两者相似且1阶相似度较高，而5和6之间不存在直连边，则两者间1阶相似度为0。

### 二阶相似(second-order proximity)

仅有1阶相似度就够了吗？显然不够，如上图，虽然5和6之间不存在直连边，但是他们有很多相同的邻居顶点(1,2,3,4)，这其实也可以表明5和6是相似的，而2阶相似度就是用来描述这种关系的。
形式化定义为
$$
p_u=(w_{u,1},...,w_{u,|v|})
$$
其中，$p_u$表示顶点$u$与其他所有顶点之间的1阶相似度，则$u$与$v$的2阶相似度可以通过$p_u$和$p_v$的相似度表示。若$u$和$v$不存在相同的邻居节点，则2阶相似度为0.

### 优化目标

- 1st

对于每一条无向边(i,j)，定义顶点$v_i$和$v_j$之间的联合概率为：
$$
p_1(v_i,v_j)=\frac{1}{1+\exp(-\vec{u}_i^T\cdot \vec{u}_i)}
$$
$\vec{u}_i$为顶点$v_i$的低维向量表示（可以看作一个内积模型，计算两个item之间的匹配程度）

同时定义经验分布
$$
\hat{p_1}=\frac{w_{ij}}{W}\\W=\sum_{(i,j)\in E}{w_{ij}}
$$
优化目标为最小化:$O_1=d(\hat{p_1}(.,.),p_1(.,.))$，$d(.,.)$是两个分布的距离，常用的衡量两个概率分布差异的指标为KL散度，使用KL散度并忽略常数项后有
$$
O_1=-\sum_{(i,j)\in E}w_{ij}\log p_1(v_i,v_j)
$$
**1st order 相似度只能用于无向图当中。**

- 2nd

这里对于每个顶点维护两个embedding向量，一个是该顶点本身的表示向量，一个是该点作为其他顶点的上下文顶点时的表示向量。(跟word2vec类似)

对于有向边(i,j)，定义给定$v_j$条件下，生成上下文邻居顶点$v_j$的概率为
$$
p_2(v_i|v_j)=\frac{\exp(\vec{c_j}^T\cdot \vec{u_i})}{\sum^{|V|}_{k=1}\exp(\vec{c_k}^T\cdot \vec{u_i})}
$$
其中，$|V|$为上下文顶点的个数。

优化目标为$O=\sum_{i\in V}\lambda _id(\overset{\land}{p_2}(.|v_i),p_2(.|v_i))$，其中$\lambda _i$为控制节点重要性的因子，可以通过顶点的度数或则PageRank等方法估计得到

经验分布定义为$\overset{\land}{p_2}(v_j|v_i)=\frac{w_{ij}}{d_i}$,$w_{ij}$为边(i,j)的边权，$d_i$是顶点$v_i$的出度，对于带权图，$d_i=\sum_{k\in N(i)}W_{ik}$

同时使用KL散度并设$\lambda _i=d_i$，忽略常数项，有
$$
O_2=-\sum_{(i,i)\in E}w_{ij}\log p_2(v_i|v_j)
$$

### 优化技巧

- Negative sampling

由于计算2阶相似度时，softmax函数的分母计算需要遍历所有顶点，这是非常低效的，论文采用了负采样优化的技巧，目标函数变为：
$$
\log\delta(\vec{c_j}^T\cdot \vec{u_i})+\sum^K_{i=1}E_{v_n\sim P_n(v)}[-\log\delta(\vec{c_n}\cdot \vec{u_i})]
$$
其中，k为负边的个数，论文使用$P_n(v)\propto d^{3/4}_v$,$d_v$是顶点$v$的出度

- Edge Sampling

注意到我们的目标函数在log之前还有一个权重系数$w_{ij}$,在使用梯度下降方法优化参数时，$w_{ij}$会直接乘在梯度上。如果图中的边权方差很大，则很难选择一个合适的学习率。若使用较大的学习率那么对于较大的边权可能会引起梯度爆炸，较小的学习率对于较小的边权则会导致梯度过小。

对于上述问题，如果所有边权相同，那么选择一个合适的学习率会变得容易。这里采用了将带权边拆分为等权边的一种方法，假如一个权重为$w$的边，则拆分后为$w$个权重为1的边。这样可以解决学习率选择的问题，但是由于边数的增长，存储的需求也会增加。

另一种方法则是从原始的带权边中进行采样，每条边被采样的概率正比于原始图中边的权重，这样既解决了学习率的问题，又没有带来过多的存储开销。

这里的采样算法使用的是Alias算法，Alias是一种O(1)时间复杂度的离散事件抽样算法。

### 其他问题

对于新加入图的顶点$v_i$ ，若该顶点与图中顶点存在边相连，我们只需要固定模型的其他参数，优化如下两个目标之一即可：
$$
-\sum_{i\in N(i)}w_{ij}\log p_1(v_j,v_i)
$$
若不存在边相连，则需要利用一些side info，留到后续工作研究。

## 2.3 Node2vec

```
node2vec是一种综合考虑DFS邻域和BFS邻域的graph embedding方法。
简单来说，可以看作是deepwalk的一种扩展，可以看作是结合了DFS和BFS随机游走的deepwalk。
```

![image-20220610093327877](./img/node2vecbfsdfs.jpg)

### 优化目标

设f(u)是将顶点u映射为embedding向量的映射函数，对于图中每个顶点u，定义$N_s(u)$为通过采样策略S采样出的顶点u的近邻顶点集合。

node2vec优化的目标是给定每个顶点条件下，令其近邻顶点出现的概率最大。
$$
\max_f \sum_{u\in V}\log P_r(N_s(U)|f(u))
$$
为了将上述最优化问题可解，文章提出两个假设：

- 条件独立性假设
  - 假设给定源顶点下，其近邻顶点出现的概率与近邻集合中其余顶点无关。
  - $P_r(N_s(U)|f(u))=\prod_{n_i\in N_s(u)}P_r(n_i|f(u))$
- 特征空间对称性假设
  - 一个顶点作为源顶点和作为近邻顶点的时候**共享同一套embedding向量**。(对比LINE中的2阶相似度，一个顶点作为源点和近邻点的时候是拥有不同的embedding向量的)
  - 在这个假设下，上诉的概率公式可表示为$P_r(n_i|f(u))=\frac{\exp{f(n_i)\cdot f(u)}}{\sum_{v\in V}\exp{f(v)\cdot f(v)}}$

根据以上两个假设条件，最终的目标函数表示为
$$
\max_f{\sum_{u\in V}[-\log{Z_u}+\sum_{n_i\in N_s(u)}f(n_i)\cdot f(u)]}
$$
由于归一化因子$Z_u=\sum_{n_i\in N_s(u)}\exp{(f(n_i)\cdot f(u))}$的计算代价太高，所以采用负采样计算优化

### 采用策略

node2vec依然采用随机游走的方式获取顶点的近邻序列，不同的是node2vec采用的是一种有偏的随机游走。

给定顶点v，访问下一个顶点x的概率为
$$
P(c_i=x|c_{i-1}=v)=\left\{ \begin{array}{l}
	\frac{\pi_{vx}}{Z}\quad if(v,x)\in E\\
	0\quad otherwise\\
\end{array} \right.
$$
$\pi_{vx}$是顶点v和顶点x之间的未归一化转移概率，Z是归一化常数。

node2vec引入两个超参数p和q来控制随机游走的策略，假设当前随机游走经过边(t,v)到达顶点v，设$\pi_{vx}=\alpha_{pq}(t,x)\cdot w_{vx}$，$w_{vx}$是顶点v和x之间的边权
$$
\alpha_{pq}(t,x)=\left\{ \begin{array}{l}
	\frac{1}{p}\quad if\quad d_{tx}=0\\
	\ 1\quad if\quad d_{tx}=1\\
	\frac{1}{q}\quad if\quad d_{tx}=2\\
\end{array} \right.
$$
$d_{tx}$为顶点t和顶点x之间的最短路径距离。

下面讨论参数p和q对游走策略的影响

- Return parameter,p
  - 参数p控制重复访问刚刚访问过的顶点的概率。
  - 注意到p仅作用于$d_{tx}=0$的情况，而$d_{tx}=0$表示顶点x就是访问当前顶点v之前访问过的顶点。
  - 若p较高，则访问刚刚访问过的顶点的概率会变低，反之变高。
- In-out papameter,q
  - q控制着游走是向外还是向内
  - 若q大于1，随机游走倾向于访问和t接近的顶点(偏向BFS)。
  - 若q小于1，随机游走倾向于访问远离t t*t*的顶点(偏向DFS)。

下面的图描述的是当从t访问到v时，决定下一个访问顶点时每个顶点对应的$\alpha$。

![image-20220610093200692](./img/node2vecsample.jpg)

### 学习算法

采样完顶点序列后，剩下的步骤就和deepwalk一样了，用word2vec去学习顶点的embedding向量。
值得注意的是node2vecWalk中不再是随机抽取邻接点，而是按概率抽取，node2vec采用了Alias算法进行顶点采样。

Alias Method:时间复杂度O(1)的离散采样方法。

![image-20220610093357506](./img/node2vec.jpg)

## 