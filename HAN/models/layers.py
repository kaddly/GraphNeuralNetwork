import torch
from torch import nn
import torch.nn.functional as F


class GraphAttentionLayer(nn.Module):
    def __init__(self, **kwargs):
        super(GraphAttentionLayer, self).__init__(**kwargs)

    def forward(self):
        pass
